{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('images.csv')\n",
    "\n",
    "# Preprocess the images\n",
    "image_directory = 'D:\\Myntra'  # Update this with the actual path to your images\n",
    "\n",
    "# Create image data generator\n",
    "datagen = ImageDataGenerator(rescale=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/shorts/rHogsxCI6gA\n",
      "[youtube] rHogsxCI6gA: Downloading webpage\n",
      "[youtube] rHogsxCI6gA: Downloading ios player API JSON\n",
      "[youtube] rHogsxCI6gA: Downloading m3u8 information\n",
      "[info] rHogsxCI6gA: Downloading 1 format(s): 18\n",
      "[download] D:\\Myntra\\vid.mp4 has already been downloaded\n",
      "[download] 100% of  928.96KiB\n",
      "Video downloaded successfully and saved to D:/Myntra\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "def download_video(url, save_path):\n",
    "    ydl_opts = {\n",
    "        'outtmpl': save_path + '/vid.%(ext)s',  # Save all videos as vid.mp4\n",
    "        'format': 'best[ext=mp4][height<=1080]',  # Download the best progressive stream available in MP4 format\n",
    "        'merge_output_format': 'mp4',  # Ensure output format is MP4 (should not need merging for progressive streams)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([url])\n",
    "        print(f'Video downloaded successfully and saved to {save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {e}')\n",
    "\n",
    "# Example usage\n",
    "video_url = 'https://www.youtube.com/shorts/rHogsxCI6gA'  # Replace with your video URL\n",
    "save_path = 'D:/Myntra'  # Replace with your desired save directory\n",
    "download_video(video_url, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def capture_screenshots(video_path, interval_sec=5, output_folder=\"screenshots\"):\n",
    "    print(f\"Capturing screenshots from video: {video_path}\")\n",
    "\n",
    "    # Check if video file exists\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Video file does not exist: {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    if os.path.exists(output_folder):\n",
    "        # Delete old screenshots if any\n",
    "        shutil.rmtree(output_folder)\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if video file was opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return\n",
    "\n",
    "    # Get frame rate of the video\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    interval_frames = interval_sec * fps\n",
    "\n",
    "    print(f\"Video frame rate: {fps} frames per second\")\n",
    "\n",
    "    frame_count = 0\n",
    "    screenshot_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % interval_frames == 0:\n",
    "            screenshot_path = f\"{output_folder}/screenshot_{screenshot_count}.jpg\"\n",
    "            cv2.imwrite(screenshot_path, frame)\n",
    "            screenshot_count += 1\n",
    "            print(f\"Saved screenshot {screenshot_count} at frame {frame_count}\")\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images and labels\n",
    "images = []\n",
    "labels = []\n",
    "for idx, row in data.iterrows():\n",
    "    img_path = os.path.join(image_directory, row['image'] + '.jpg')  # Assuming images are in jpg format\n",
    "    if os.path.exists(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        images.append(img)\n",
    "        labels.append(row['label'])\n",
    "\n",
    "images = np.array(images)\n",
    "labels = pd.get_dummies(labels).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\icyga\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.0783 - loss: 66.0700 - val_accuracy: 0.1692 - val_loss: 2.8582\n",
      "Epoch 2/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 160ms/step - accuracy: 0.2381 - loss: 2.7066 - val_accuracy: 0.2538 - val_loss: 2.7325\n",
      "Epoch 3/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 154ms/step - accuracy: 0.2653 - loss: 2.5461 - val_accuracy: 0.2385 - val_loss: 2.7421\n",
      "Epoch 4/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 155ms/step - accuracy: 0.2957 - loss: 2.5015 - val_accuracy: 0.2846 - val_loss: 2.5227\n",
      "Epoch 5/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 165ms/step - accuracy: 0.4081 - loss: 2.0250 - val_accuracy: 0.3154 - val_loss: 2.5279\n",
      "Epoch 6/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 173ms/step - accuracy: 0.4730 - loss: 1.8049 - val_accuracy: 0.3308 - val_loss: 2.5192\n",
      "Epoch 7/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 166ms/step - accuracy: 0.5311 - loss: 1.6566 - val_accuracy: 0.3462 - val_loss: 2.4128\n",
      "Epoch 8/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 171ms/step - accuracy: 0.5580 - loss: 1.4074 - val_accuracy: 0.3462 - val_loss: 2.5854\n",
      "Epoch 9/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 163ms/step - accuracy: 0.6926 - loss: 0.9607 - val_accuracy: 0.3769 - val_loss: 2.5632\n",
      "Epoch 10/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 160ms/step - accuracy: 0.7478 - loss: 0.8440 - val_accuracy: 0.3692 - val_loss: 2.7696\n",
      "Epoch 11/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 163ms/step - accuracy: 0.7728 - loss: 0.7196 - val_accuracy: 0.3692 - val_loss: 2.7224\n",
      "Epoch 12/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 168ms/step - accuracy: 0.8482 - loss: 0.4900 - val_accuracy: 0.3615 - val_loss: 2.8739\n",
      "Epoch 13/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 170ms/step - accuracy: 0.8234 - loss: 0.6192 - val_accuracy: 0.3154 - val_loss: 2.9001\n",
      "Epoch 14/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 162ms/step - accuracy: 0.8661 - loss: 0.5671 - val_accuracy: 0.3308 - val_loss: 3.1942\n",
      "Epoch 15/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 179ms/step - accuracy: 0.8407 - loss: 0.5171 - val_accuracy: 0.3538 - val_loss: 3.1353\n",
      "Epoch 16/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 176ms/step - accuracy: 0.8368 - loss: 0.5109 - val_accuracy: 0.3462 - val_loss: 3.1528\n",
      "Epoch 17/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 174ms/step - accuracy: 0.9057 - loss: 0.3531 - val_accuracy: 0.3462 - val_loss: 3.3021\n",
      "Epoch 18/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 180ms/step - accuracy: 0.8825 - loss: 0.4218 - val_accuracy: 0.3462 - val_loss: 3.2425\n",
      "Epoch 19/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 185ms/step - accuracy: 0.9365 - loss: 0.2771 - val_accuracy: 0.3231 - val_loss: 3.3888\n",
      "Epoch 20/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 166ms/step - accuracy: 0.9179 - loss: 0.3731 - val_accuracy: 0.4000 - val_loss: 2.9199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(labels[0]), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "model.save('clothing_model3.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "from tensorflow.keras.models import load_model\n",
    "import webcolors\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "loaded_model = load_model('clothing_model3.h5')\n",
    "\n",
    "# Function to detect clothing type\n",
    "def detect_clothing_type(image_path, model):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_label = np.argmax(prediction, axis=1)\n",
    "    label_map = {i: l for i, l in enumerate(data['label'].unique())}\n",
    "    return label_map[predicted_label[0]]\n",
    "\n",
    "# Predefined color dictionary\n",
    "COLORS = {\n",
    "    \"black\": (0, 0, 0), \"white\": (255, 255, 255), \"red\": (255, 0, 0), \n",
    "    \"lime\": (0, 255, 0), \"blue\": (0, 0, 255), \"yellow\": (255, 255, 0), \n",
    "    \"cyan\": (0, 255, 255), \"magenta\": (255, 0, 255), \"silver\": (192, 192, 192), \n",
    "    \"gray\": (128, 128, 128), \"maroon\": (128, 0, 0), \"olive\": (128, 128, 0), \n",
    "    \"green\": (0, 128, 0), \"purple\": (128, 0, 128), \"teal\": (0, 128, 128), \n",
    "    \"navy\": (0, 0, 128)\n",
    "}\n",
    "\n",
    "\n",
    "def closest_color(requested_color):\n",
    "    min_colors = {}\n",
    "    for name, rgb in COLORS.items():\n",
    "        rd = (rgb[0] - requested_color[0]) ** 2\n",
    "        gd = (rgb[1] - requested_color[1]) ** 2\n",
    "        bd = (rgb[2] - requested_color[2]) ** 2\n",
    "        min_colors[(rd + gd + bd)] = name\n",
    "    return min_colors[min(min_colors.keys())]\n",
    "\n",
    "def detect_color(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    data = np.reshape(img, (-1, 3))\n",
    "    data = np.float32(data)\n",
    "\n",
    "    # Apply k-means clustering to find the dominant color\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "    k = 1\n",
    "    _, label, center = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    center = np.uint8(center)\n",
    "    dominant_color = center[0]\n",
    "\n",
    "    # Find the closest named color\n",
    "    color_name = closest_color(dominant_color)\n",
    "    return color_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing screenshots from video: vid.mp4\n",
      "Video frame rate: 30 frames per second\n",
      "Saved screenshot 1 at frame 0\n",
      "Saved screenshot 2 at frame 150\n",
      "Saved screenshot 3 at frame 300\n"
     ]
    }
   ],
   "source": [
    "capture_screenshots('vid.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Predicted clothing type for screenshot_0.jpg: Dress, Dominant color: silver\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Predicted clothing type for screenshot_1.jpg: Dress, Dominant color: gray\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Predicted clothing type for screenshot_2.jpg: Body, Dominant color: silver\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "\n",
    "    # Define class labels (from your training phase)\n",
    "    class_indices = {'Blazer': 0, 'Blouse': 1, 'Body': 2, 'Dress': 3, 'Hat': 4,  'Longsleeve': 6, 'Not sure': 7, 'Other': 8, 'Outwear': 9, 'Pants': 10, 'Polo': 11, 'Shirt': 12, 'Shoes': 13, 'Shorts': 14, 'Skip': 15, 'Skirt': 16, 'T-Shirt': 17, 'Top': 18, 'Undershirt': 19}\n",
    "    class_labels = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "    # Predict clothing types and colors for captured screenshots\n",
    "    screenshot_folder = \"screenshots\"\n",
    "    best_prediction = None\n",
    "    highest_confidence = 0\n",
    "\n",
    "    for screenshot in os.listdir(screenshot_folder):\n",
    "        if screenshot.endswith('.jpg'):\n",
    "            screenshot_path = os.path.join(screenshot_folder, screenshot)\n",
    "            predicted_label= detect_clothing_type(screenshot_path, loaded_model)\n",
    "            dominant_color = detect_color(screenshot_path)\n",
    "            print(f\"Predicted clothing type for {screenshot}: {predicted_label}, Dominant color: {dominant_color}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Clothing type: Pants\n",
      "Color: silver\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "image_path = '2058.jpg'\n",
    "clothing_type = detect_clothing_type(image_path, loaded_model)\n",
    "color = detect_color(image_path)\n",
    "\n",
    "print(f\"Clothing type: {clothing_type}\")\n",
    "print(f\"Color: {color}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
